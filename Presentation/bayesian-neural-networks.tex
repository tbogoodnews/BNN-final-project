\documentclass{beamer}
\usetheme{Madrid}

\usepackage{multicol}
\definecolor{ReedRed}{RGB}{167, 14, 22} %Reed Red (primary)
\usecolortheme[named=ReedRed]{structure}
\useoutertheme{miniframes}
\useinnertheme{circles}
\usepackage{soul}


\usepackage{listings}


\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{snakes}
\usetikzlibrary{positioning}
\usepackage{rotating}

\usepackage[citestyle=apa,backend=biber,citestyle=numeric]{biblatex}
\addbibresource{../refs.bib}

\title{Bayesian Neural Networks}
\author{
	Ava, Conor, Taylor
}
\logo{
	\includegraphics[width=1cm]{../Images/reed-logo.png}
}
\institute{Reed College}
\setbeamertemplate{navigation symbols}{} % Remove nav bar

\begin{document}

	
\begin{frame}[plain]
    \maketitle
\end{frame}


\section{Neural Networks}

\begin{frame}{Neural Networks (NN)}
	% A convolutional neural network has the same structure as a regular neural network
	% Except we slide over the image and take k
	\begin{figure}
	\begin{tikzpicture}
		% Define node styles
		\tikzstyle{neuron} = [circle, draw, minimum size=1.5cm];
		\tikzstyle{weight} = [font=\scriptsize];
		
		% Input layer (2 neurons)
		\node[neuron] (input1) at (-2,1) {Input 1};
		\node[neuron] (input2) at (-2,-1) {Input 2};
		
		% Hidden layer (3 neurons)
		\node[neuron] (hidden1) at (2,2) {Hidden 1};
		\node[neuron] (hidden2) at (2,0) {Hidden 2};
		\node[neuron] (hidden3) at (2,-2) {Hidden 3};
		
		% Output layer (2 neurons for classification)
		\node[neuron] (output1) at (6,1) {Class 1};
		\node[neuron] (output2) at (6,-1) {Class 2};
		
		% Connections with weights
		\draw[->] (input1) -- node[weight, above] {$w_{1,1}$} (hidden1);
		\draw[->] (input2) -- node[weight, above] {$w_{2,1}$} (hidden1);
		\draw[->] (input1) -- node[weight, right] {$w_{1,2}$} (hidden2);
		\draw[->] (input2) -- node[weight, right] {$w_{2,2}$} (hidden2);
		\draw[->] (input1) -- node[weight, above] {$w_{1,3}$} (hidden3);
		\draw[->] (input2) -- node[weight, above] {$w_{2,3}$} (hidden3);
		\draw[->] (hidden1) -- node[weight, right] {$v_{1,1}$} (output1);
		\draw[->] (hidden2) -- node[weight, right] {$v_{2,1}$} (output1);
		\draw[->] (hidden3) -- node[weight, right] {$v_{3,1}$} (output1);
		\draw[->] (hidden1) -- node[weight, above] {$v_{12}$} (output2);
		\draw[->] (hidden2) -- node[weight, above] {$v_{2,2}$} (output2);
		\draw[->] (hidden3) -- node[weight, above] {$v_{3,2}$} (output2);
	\end{tikzpicture}
		\caption{Example neural network}
	\end{figure}
\end{frame}

\begin{frame}{A Neural Network Neuron}
	% A convolutional neural network has the same structure as a regular neural network
	% Except we slide over the image and take k
	\begin{figure}
		\begin{tikzpicture}[
			neuron/.style={
				circle,
				draw,
				minimum size=1.5em,
				inner sep=0pt,
			},
			font=\footnotesize,
			>=latex
			]
			% Draw the neuron
			\node [neuron] (neuron) at (0,0) {$\Sigma$};
			% Draw the input nodes and labels
			\foreach \i/\txt in {1/$1$,2/$2$,3/$n$} {
				\node [neuron,left=of neuron] (input\i) at (-1.5,-\i +2) {x\txt};
				\draw[->] (input\i) -- node[above] {$w$\txt} (neuron);
			}
			% Draw the bias node and label
			\node [neuron,left=of neuron] (bias) at (1.25,1) {Bias};
			\draw[->] (bias) -- node[right] {} (neuron);
			% Draw the activation function label
			\node[align=center,below right =0.5em of neuron] {Activation \\ function};
			% Draw the output node and label
			\node [neuron,right=of neuron] (output) at (1.5,0) {Output};
			\draw[->] (neuron) -- node[above] {} (output);
			% Draw the relevant functions
			%\draw[->] (0,-1.5) -- (0,-2.5) node[midway,right] {Weighted sum};
			%\draw[->] (-1.5,-3.5) -- (-1.5,-4.5) node[midway,left] {Bias};
			%\draw[->] (1.5,-0.5) -- (1.5,-1.5) node[midway,right] {Activation};
			
			% Draw the activation function shape
			%\node[draw,rectangle,minimum width=2em,minimum height=2em] (actfunc) at (0,1) {};
			%\node[above=0.1em of actfunc] {ReLU};
			
		\end{tikzpicture}
		\caption{Example neural network neuron}
	\end{figure}
\end{frame}

\begin{frame}{Issues with Neural Networks}
	\begin{multicols}{2}
		\begin{figure}
			\includegraphics[width=.4\textwidth]{../Images/xkcd_machine_learning.png}
			\caption{XKCD: "\textit{Machine Learning}" \cite{xkcd-ml}}
		\end{figure}
		
		\columnbreak
		
		\null \vfill
		\begin{itemize}
			\item Stir data and pray
			\item Interpretability problems
			\item Lots of data required
			\item Risk of overfitting
			\item Unpredictable failures to generalize
			\item No uncertainty quantification
			\item Computationally expensive
		\end{itemize}
		\vfill \null
	\end{multicols}
\end{frame}

\begin{frame}{Convolutional Neural Networks (CNN)}
	% A convolutional neural network has the same structure as a regular neural network
	% Except we slide over the image and take k
	\begin{figure}
\begin{tikzpicture}[draw=black!50, node distance=0.5cm]
	% Input matrix
	\draw[step=0.5cm,color=black!30] (0,0) grid (3,3);
	\foreach \x in {0,1,2}
	\foreach \y in {0,1,2}
	\node at (\x*0.5+0.25,\y*0.5+0.25) {$x_{\x,\y}$};
	
	% Convolutional kernel
	\draw[step=0.5cm,color=red,thick] (0,0) grid (2,2);
	\foreach \x in {0,1}
	\foreach \y in {0,1}
	\node[fill=red!30,minimum size=0.5cm] at (\x*0.5+0.25,\y*0.5+0.25) {};
	
	% Output feature map
	\begin{scope}[shift={(4,0)}]
		\draw[step=0.5cm,color=black!30] (0,0) grid (2,2);
		\foreach \x in {0,1}
		\foreach \y in {0,1}
		\node[fill=blue!30,minimum size=0.5cm] at (\x*0.5+0.25,\y*0.5+0.25) {};
	\end{scope}
	
	% Sliding operation
	\draw[->,thick] (2,1) -- (3.5,1) node[midway,above] {Sliding};
	
	% Annotations
	\node[below=of current bounding box.south] {Input Matrix};
	\node[below=of current bounding box.south,shift={(4,0)}] {Output Feature Map};
	\node at (0,-1) {Convolutional Kernel};
\end{tikzpicture}
	\end{figure}
\end{frame}

\begin{frame}{Convolutional Neural Networks (CNN)}
	% A convolutional neural network has the same structure as a regular neural network
	% Except we slide over the image and take k
	\begin{figure}
		\includegraphics[width=.9\textwidth]{../Images/big-pic-cnn.jpg}
		\caption{CNN pipeline \cite{eli5CNN}}
	\end{figure}
\end{frame}


\begin{frame}{Why we use CNNs}
	\begin{multicols}{2}
		\begin{figure}
			\includegraphics[width=.45\textwidth]{../Images/xkcd_self_driving.png}
			\caption{XKCD: "\textit{Self Driving}" \cite{xkcd-self-driving}}
		\end{figure}
		
		\columnbreak
		
		\null \vfill
		\begin{itemize}
			\item Fewer parameters
			\item Encode spatial patterns
			\item More efficient for image tasks
			%\item Channels...
		\end{itemize}
		\vfill \null
	\end{multicols}
\end{frame}

\section{Bayesian Neural Networks}

\begin{frame}{Bayesian Neural Network}
	% The relationship between BNN and CNNs is the same as NNs and CNNs
	\begin{figure}
		\includegraphics[width=.55\textwidth]{../Images/example_bnn.png}
		\caption{Example BNN  \cite{FleszarBNN}}
	\end{figure}
\end{frame}

\begin{frame}{BNN Neuron}
	% The relationship between BNN and CNNs is the same as NNs and CNNs
	\begin{figure}
		\includegraphics[width=.55\textwidth]{../Images/BNN-neuron.png}
		\caption{Example BNN Neuron \cite{hase2019machine}}
	\end{figure}
\end{frame}

\begin{frame}{Why we use BNN}
	\begin{multicols}{2}
		\begin{figure}
			\includegraphics[width=.45\textwidth]{../Images/xkcd_error_bars.png}
			\caption{XKCD: "\textit{Error Bars}" \cite{xkcd-self-driving}}
		\end{figure}
		
		\columnbreak
		
		\null \vfill
		\begin{itemize}
			\item Well-calibrated uncertainty
			\item Handles sparse data while minimizing overfitting
			\item More predictable failures
			\item Formalizes prior knowledge and assumptions
			\item Inherent sequentiality
		\end{itemize}
		\vfill \null
	\end{multicols}
\end{frame}

\begin{frame}{Applications}
	\begin{multicols}{2}
		\begin{figure}
			\includegraphics[width=.4\textwidth]{../Images/xkcd_simple_answers.png}
			\caption{XKCD: "\textit{Simple Answers}" \cite{xkcd-simple-answers}}
		\end{figure}
		
		\columnbreak
		
		\null \vfill
		\begin{itemize}
			\item Uncertainty quantification
				\begin{itemize}
					\item Engineering, Medicine, Finance, ...
				\end{itemize}
			\item Sparse data
				\begin{itemize}
					\item Anywhere data is expensive
					\item Medical diagnosis
					\item Molecular biology
				\end{itemize}
			\item Warnings before failing to generalize
				\begin{itemize}
					\item Autonomous driving
					\item Engineering
				\end{itemize}

			\item Sequentiality
		\end{itemize}
		
		\vfill \null
	\end{multicols}
\end{frame}

\begin{frame}{Difference between BNNs and BCNNs}
	% The relationship between BNN and CNNs is the same as NNs and CNNs
	\begin{figure}
		\includegraphics[width=.75\textwidth]{../Images/the_general_problem.png}
		\caption{ XKCD: "\textit{The General Problem}" \cite{xkcd-general-problem}}
	\end{figure}
		The relationship between BNNs and BCNNs is the same as NNs and CNNs.
\end{frame}


\section{Simulation}

\begin{frame}{CIFAR-10}
	% A convolutional neural network has the same structure as a regular neural network
	% Except we slide over the image and take k
	\begin{figure}
		\includegraphics[width=.65\textwidth]{../Images/cifar-10.png}
		\caption{Example CIFAR-10 images \cite{cifar10}}
	\end{figure}
\end{frame}

\begin{frame}{Hyperparameters}
	% Replace cell text later
	\centering
	\begin{tabular}{|c||p{3cm}|p{3cm}|} % Adjust p{width} as needed
		\hline
		\textbf{Hyperparameter} & \textbf{CNN} & \textbf{BCNN} \\ [0.5ex] 
		\hline\hline
		Epochs & 100 & 100\\
		\hline
		Learning Rate & 0.001  & 0.003  \\
		\hline
		Regularization Rate& 0.001 & 0.001 \\
		\hline
		Optimizer & Adamw  & Adamw  \\
		\hline
	\end{tabular}
\end{frame}

\begin{frame}{Results}
	% Image Later
	\centering
	\begin{tabular}{|c||p{3cm}|p{3cm}|} % Adjust p{width} as needed
		\hline
		\textbf{Metric} & \textbf{CNN} & \textbf{BCNN} \\ [0.5ex] 
		\hline\hline
		Train Accuracy & 84.96\% & 81.27\%\\
		\hline
		Validation Accuracy & 61.76\%  & 59.21\%  \\
		\hline
		Time to Train & 16 min 11 sec  & 22 min 11 sec  \\
		\hline
	\end{tabular}
\end{frame}

\begin{frame}{Accuracy over time (CNN)}
	% Image Later
	\begin{figure}
		\includegraphics[width=.75\textwidth]{../Images/CNN_val_acc_over_time}
	\end{figure}
\end{frame}

\begin{frame}{Accuracy over time (BNN)}
	% Image Later
	\begin{figure}
		\includegraphics[width=.75\textwidth]{../Images/BNN_val_acc_over_time}
	\end{figure}
\end{frame}

\begin{frame}{Confusion Matrix (CNN)}
	% Image Later
	\begin{figure}
		\includegraphics[width=.95\textwidth]{../Images/CNN_confusion_matrix.png}
	\end{figure}
\end{frame}

\begin{frame}{Confusion Matrix (BCNN)}
	% Image Later
	\begin{figure}
		\includegraphics[width=.95\textwidth]{../Images/BNN_confusion_matrix.png}
	\end{figure}
\end{frame}

\begin{frame}{Live Demo}
	\begin{figure}
		\includegraphics[width=.35\textwidth]{../Images/xkcd_laws_of_physics.png}
		\caption{XKCD: "\textit{Laws of Physics}" \cite{xkcd-laws-of-physics}}
	\end{figure}
\end{frame}

\section{Closing}

\begin{frame}{Questions}
	\begin{figure}
		\includegraphics[width=.275\textwidth]{../Images/conference_question.png}
		\caption{XKCD: "\textit{Conference Question}" \cite{xkcd-confrence-questions}}
	\end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{References}
\printbibliography
\end{frame}

\end{document}


